{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68268195-8a46-434a-b6bd-31f2f5e1cc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "# Replace 'your_huggingface_token' with your actual token\n",
    "HUGGINGFACE_TOKEN = \"\"\n",
    "\n",
    "# Authenticate with Hugging Face\n",
    "login(HUGGINGFACE_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f95cab3-fb94-4eeb-ab97-338fca3d1010",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb7e67d9689b477390acd65e15bfd044",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 8,388,608 || all params: 6,746,804,224 || trainable%: 0.1243\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "from torch.optim import AdamW\n",
    "from datasets import Dataset\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from datasets import Dataset, load_dataset\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "def set_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(2025)\n",
    "\n",
    "model_name = \"Meta-Llama/Llama-2-7b-chat-hf\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Configure 4-bit quantization using bitsandbytes\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",  # Normalized Float 4 (better than standard FP4)\n",
    "    bnb_4bit_use_double_quant=True,  # Uses secondary quantization for better precision\n",
    "    bnb_4bit_compute_dtype=torch.float16  # Keeps computation in FP16 for stability\n",
    ")\n",
    "\n",
    "# uncomment these first time\n",
    "# Load LLaMA 2.7B with 4-bit quantization\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "base_model.config.pad_token_id = tokenizer.eos_token_id  # Set pad token ID\n",
    "\n",
    "# Configure LoRA for memory-efficient fine-tuning\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=['q_proj', 'k_proj', 'v_proj', 'o_proj']  # Apply LoRA to attention layers\n",
    ")\n",
    "\n",
    "# Wrap the model with LoRA adapters\n",
    "model = get_peft_model(base_model, lora_config)\n",
    "model.print_trainable_parameters()  # Verify LoRA trainable parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c646c183-25de-42c8-a61f-f9c3ff6d2c43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1500, 500)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import random\n",
    "\n",
    "ds = load_dataset(\"rewoo/planner_instruction_tuning_2k\")\n",
    "dataset = ds['train']\n",
    "\n",
    "# Shuffle and select 1000 samples for training\n",
    "train_dataset = dataset.shuffle(seed=42).select(range(1500))\n",
    "\n",
    "# Shuffle again (independently) and select 500 samples for testing\n",
    "validation_dataset = dataset.shuffle(seed=123).select(range(500))\n",
    "len(train_dataset), len(validation_dataset) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1e20953-0ff5-47cd-9f06-e0839b858767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing function\n",
    "def preprocess_data(example):\n",
    "    input_text = f\"Instruction: {example['instruction']}\\nInput: {example['input']}\\nOutput: {example['output']}\"\n",
    "    inputs = tokenizer(input_text, truncation=True, padding=\"max_length\", max_length=512)\n",
    "\n",
    "    # Copy input_ids to labels\n",
    "    labels = inputs[\"input_ids\"].copy()\n",
    "\n",
    "    # Mask question tokens and padding tokens in labels\n",
    "    question_length = len(tokenizer(f\"Instruction: {example['instruction']}\\nInput: {example['input']}\\nOutput:\")[\"input_ids\"]) - 1\n",
    "    for i in range(len(labels)):\n",
    "        if i < question_length or labels[i] == tokenizer.pad_token_id:\n",
    "            labels[i] = -100  # Ignore these tokens in loss computation\n",
    "\n",
    "    inputs[\"labels\"] = labels\n",
    "    return inputs\n",
    "\n",
    "# DataLoader Collation\n",
    "def collate_fn(batch):\n",
    "    input_ids = torch.tensor([item[\"input_ids\"] for item in batch], dtype=torch.long)\n",
    "    attention_mask = torch.tensor([item[\"attention_mask\"] for item in batch], dtype=torch.long)\n",
    "    labels = torch.tensor([item[\"labels\"] for item in batch], dtype=torch.long)\n",
    "    return input_ids, attention_mask, labels\n",
    "\n",
    "# Convert samples to dataset and preprocess\n",
    "    \n",
    "dataset_train = train_dataset.map(preprocess_data, remove_columns=train_dataset.column_names)\n",
    "dataset_valid = validation_dataset.map(preprocess_data, remove_columns=validation_dataset.column_names)\n",
    "\n",
    "# Convert to PyTorch DataLoader\n",
    "batch_size = 4\n",
    "\n",
    "train_loader = DataLoader(dataset_train, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "valid_loader = DataLoader(dataset_valid, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "640dcf7b-1f02-4326-85ea-98544eb89dbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Train Loss: 0.1962, Val Loss: 0.1257\n",
      "✅ Best model saved at epoch 1 with validation loss: 0.1257\n",
      "Epoch 2/3, Train Loss: 0.1174, Val Loss: 0.1073\n",
      "✅ Best model saved at epoch 2 with validation loss: 0.1073\n",
      "Epoch 3/3, Train Loss: 0.0974, Val Loss: 0.0966\n",
      "✅ Best model saved at epoch 3 with validation loss: 0.0966\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Training Function\n",
    "def train(model, train_loader, valid_loader, optimizer, criterion, num_epochs=3):\n",
    "    best_val_loss = float(\"inf\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            input_ids, attention_mask, labels = [x.to(device) for x in batch]\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward Pass\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "\n",
    "            # Shift logits and labels for loss computation\n",
    "            shift_logits = logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "            shift_logits = shift_logits.view(-1, shift_logits.size(-1))\n",
    "            shift_labels = shift_labels.view(-1)\n",
    "\n",
    "            # Compute Loss\n",
    "            loss = criterion(shift_logits, shift_labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        avg_val_loss = validate(model, valid_loader, criterion)\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "        # Save best model\n",
    "        best_val_loss = save_best_model(model, tokenizer, epoch + 1, best_val_loss, avg_val_loss)\n",
    "\n",
    "# Validation Function\n",
    "def validate(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids, attention_mask, labels = [x.to(device) for x in batch]\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "\n",
    "            shift_logits = logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "            shift_logits = shift_logits.view(-1, shift_logits.size(-1))\n",
    "            shift_labels = shift_labels.view(-1)\n",
    "\n",
    "            loss = criterion(shift_logits, shift_labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "\n",
    "# Function to save the best model\n",
    "def save_best_model(model, tokenizer, epoch, best_loss, current_loss, save_path=\"./lora/bestmodel\"):\n",
    "    if current_loss < best_loss:\n",
    "        best_loss = current_loss\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "        model.save_pretrained(save_path)\n",
    "        tokenizer.save_pretrained(save_path)\n",
    "        print(f\"✅ Best model saved at epoch {epoch} with validation loss: {best_loss:.4f}\")\n",
    "    return best_loss\n",
    "\n",
    "\n",
    "\n",
    "# Optimizer & Loss Function\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=-100)\n",
    "\n",
    "# Start Training\n",
    "train(model, train_loader, valid_loader, optimizer, criterion, num_epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368958ea-819f-48a7-8a02-46b3e2077273",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
