{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2c32190-0325-4ef7-bef6-5c90ac4c2aae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sa5u24/agent/AgentTuning/eval_heldout/rewoo\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Replace 'your-hf-token-here' with your actual Hugging Face token\n",
    "login(token=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b620d3a7-1624-496e-aa20-c6ddf5f87424",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1500,\n",
       " 200,\n",
       " {'instruction': 'For the following tasks, make plans that can solve the problem step-by-step. For each plan, indicate which external tool together with tool input to retrieve evidence. You can store the evidence into a variable #E that can be called by later tools. (Plan, #E1, Plan, #E2, Plan, ...)\\n\\nTools can be one of the following:\\nWikipedia[input]: Worker that search for similar page contents from Wikipedia. Useful when you need to get holistic knowledge about people, places, companies, historical events, or other subjects. The response are long and might contain some irrelevant information. Input should be a search query.\\nLLM[input]: A pretrained LLM like yourself. Useful when you need to act with general world knowledge and common sense. Prioritize it when you are confident in solving the problem yourself. Input can be any instruction.',\n",
       "  'input': 'Which model of Ford car took its name from an Italian alpine resort?',\n",
       "  'output': 'Plan: Search for more information about Ford car models\\n#E1 = Wikipedia[Ford car models]\\nPlan: Search for more information about Italian alpine resorts\\n#E2 = Wikipedia[Italian alpine resorts]\\nPlan: Find out the name of the Ford car model that takes its name from an Italian alpine resort.\\n#E3 = LLM[What is the name of the Ford car model that takes its name from an Italian alpine resort? Given context: #E1 and #E2]'})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Set a seed for reproducibility\n",
    "random.seed(42)\n",
    "\n",
    "ds = load_dataset(\"rewoo/planner_instruction_tuning_2k\")\n",
    "dataset = ds['train']\n",
    "\n",
    "# Shuffle and select 1000 samples for training\n",
    "train_dataset = dataset.shuffle(seed=42).select(range(1500))\n",
    "\n",
    "# Shuffle again (independently) and select 500 samples for testing\n",
    "validation_dataset = dataset.shuffle(seed=123).select(range(200))\n",
    "\n",
    "sampled_data = validation_dataset\n",
    "len(train_dataset), len(validation_dataset), sampled_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b15e00ab-8225-4fb6-b94a-eab923fe9a31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2e36f413f224d07b5aa825a12874f05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Configure 4-bit quantization using bitsandbytes\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",  # Normalized Float 4 (better than standard FP4)\n",
    "    bnb_4bit_use_double_quant=True,  # Uses secondary quantization for better precision\n",
    "    bnb_4bit_compute_dtype=torch.float16  # Keeps computation in FP16 for stability\n",
    ")\n",
    "\n",
    "# Meta-Llama/Llama-2-7b-chat-hf\n",
    "# meta-llama/Llama-3.2-1B-Instruct\n",
    "# Load the base LLaMA 2.7B model\n",
    "base_model = LlamaForCausalLM.from_pretrained(\n",
    "    \"Meta-Llama/Llama-2-7b-chat-hf\",\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "best_model_path = \"./lora/bestmodel\"\n",
    "# # Load the LoRA fine-tuned adapter\n",
    "model_peft = PeftModel.from_pretrained(base_model, best_model_path)\n",
    "\n",
    "\n",
    "# Load the fine-tuned tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(best_model_path, use_fast=False)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Ensure correct padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f091d091-780d-4005-b180-bfb555898e0a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Layer 0 | q_proj | SpSVD successful | shape: torch.Size([4096, 4096])\n",
      "✅ Layer 0 | k_proj | SpSVD successful | shape: torch.Size([4096, 4096])\n",
      "✅ Layer 0 | v_proj | SpSVD successful | shape: torch.Size([4096, 4096])\n",
      "✅ Layer 0 | o_proj | SpSVD successful | shape: torch.Size([4096, 4096])\n",
      "✅ Layer 1 | q_proj | SpSVD successful | shape: torch.Size([4096, 4096])\n",
      "✅ Layer 1 | k_proj | SpSVD successful | shape: torch.Size([4096, 4096])\n",
      "✅ Layer 1 | v_proj | SpSVD successful | shape: torch.Size([4096, 4096])\n",
      "✅ Layer 1 | o_proj | SpSVD successful | shape: torch.Size([4096, 4096])\n",
      "✅ Layer 2 | q_proj | SpSVD successful | shape: torch.Size([4096, 4096])\n",
      "✅ Layer 2 | k_proj | SpSVD successful | shape: torch.Size([4096, 4096])\n",
      "✅ Layer 2 | v_proj | SpSVD successful | shape: torch.Size([4096, 4096])\n",
      "✅ Layer 2 | o_proj | SpSVD successful | shape: torch.Size([4096, 4096])\n",
      "✅ Layer 3 | q_proj | SpSVD successful | shape: torch.Size([4096, 4096])\n",
      "✅ Layer 3 | k_proj | SpSVD successful | shape: torch.Size([4096, 4096])\n",
      "✅ Layer 3 | v_proj | SpSVD successful | shape: torch.Size([4096, 4096])\n",
      "✅ Layer 3 | o_proj | SpSVD successful | shape: torch.Size([4096, 4096])\n",
      "✅ Layer 4 | q_proj | SpSVD successful | shape: torch.Size([4096, 4096])\n",
      "✅ Layer 4 | k_proj | SpSVD successful | shape: torch.Size([4096, 4096])\n",
      "✅ Layer 4 | v_proj | SpSVD successful | shape: torch.Size([4096, 4096])\n",
      "✅ Layer 4 | o_proj | SpSVD successful | shape: torch.Size([4096, 4096])\n",
      "✅ Layer 5 | q_proj | SpSVD successful | shape: torch.Size([4096, 4096])\n",
      "✅ Layer 5 | k_proj | SpSVD successful | shape: torch.Size([4096, 4096])\n",
      "✅ Layer 5 | v_proj | SpSVD successful | shape: torch.Size([4096, 4096])\n",
      "✅ Layer 5 | o_proj | SpSVD successful | shape: torch.Size([4096, 4096])\n",
      "✅ Layer 6 | q_proj | SpSVD successful | shape: torch.Size([4096, 4096])\n",
      "✅ Layer 6 | k_proj | SpSVD successful | shape: torch.Size([4096, 4096])\n",
      "✅ Layer 6 | v_proj | SpSVD successful | shape: torch.Size([4096, 4096])\n",
      "✅ Layer 6 | o_proj | SpSVD successful | shape: torch.Size([4096, 4096])\n",
      "✅ Layer 7 | q_proj | SpSVD successful | shape: torch.Size([4096, 4096])\n",
      "✅ Layer 7 | k_proj | SpSVD successful | shape: torch.Size([4096, 4096])\n",
      "✅ Layer 7 | v_proj | SpSVD successful | shape: torch.Size([4096, 4096])\n",
      "✅ Layer 7 | o_proj | SpSVD successful | shape: torch.Size([4096, 4096])\n",
      "✅ Layer 8 | q_proj | SpSVD successful | shape: torch.Size([4096, 4096])\n",
      "✅ Layer 8 | k_proj | SpSVD successful | shape: torch.Size([4096, 4096])\n",
      "✅ Layer 8 | v_proj | SpSVD successful | shape: torch.Size([4096, 4096])\n",
      "✅ Layer 8 | o_proj | SpSVD successful | shape: torch.Size([4096, 4096])\n",
      "✅ Layer 9 | q_proj | SpSVD successful | shape: torch.Size([4096, 4096])\n",
      "✅ Layer 9 | k_proj | SpSVD successful | shape: torch.Size([4096, 4096])\n",
      "✅ Layer 9 | v_proj | SpSVD successful | shape: torch.Size([4096, 4096])\n",
      "✅ Layer 9 | o_proj | SpSVD successful | shape: torch.Size([4096, 4096])\n",
      "✅ Layer 10 | q_proj | SpSVD successful | shape: torch.Size([4096, 4096])\n",
      "✅ Layer 10 | k_proj | SpSVD successful | shape: torch.Size([4096, 4096])\n",
      "✅ Layer 10 | v_proj | SpSVD successful | shape: torch.Size([4096, 4096])\n",
      "✅ Layer 10 | o_proj | SpSVD successful | shape: torch.Size([4096, 4096])\n",
      "✅ Layer 11 | q_proj | SpSVD successful | shape: torch.Size([4096, 4096])\n",
      "✅ Layer 11 | k_proj | SpSVD successful | shape: torch.Size([4096, 4096])\n",
      "✅ Layer 11 | v_proj | SpSVD successful | shape: torch.Size([4096, 4096])\n",
      "✅ Layer 11 | o_proj | SpSVD successful | shape: torch.Size([4096, 4096])\n",
      "✅ Layer 12 | q_proj | SpSVD successful | shape: torch.Size([4096, 4096])\n",
      "✅ Layer 12 | k_proj | SpSVD successful | shape: torch.Size([4096, 4096])\n",
      "✅ Layer 12 | v_proj | SpSVD successful | shape: torch.Size([4096, 4096])\n",
      "✅ Layer 12 | o_proj | SpSVD successful | shape: torch.Size([4096, 4096])\n",
      "✅ Layer 13 | q_proj | SpSVD successful | shape: torch.Size([4096, 4096])\n",
      "✅ Layer 13 | k_proj | SpSVD successful | shape: torch.Size([4096, 4096])\n",
      "✅ Layer 13 | v_proj | SpSVD successful | shape: torch.Size([4096, 4096])\n",
      "✅ Layer 13 | o_proj | SpSVD successful | shape: torch.Size([4096, 4096])\n",
      "✅ Layer 14 | q_proj | SpSVD successful | shape: torch.Size([4096, 4096])\n",
      "✅ Layer 14 | k_proj | SpSVD successful | shape: torch.Size([4096, 4096])\n",
      "✅ Layer 14 | v_proj | SpSVD successful | shape: torch.Size([4096, 4096])\n",
      "✅ Layer 14 | o_proj | SpSVD successful | shape: torch.Size([4096, 4096])\n",
      "✅ Layer 15 | q_proj | SpSVD successful | shape: torch.Size([4096, 4096])\n",
      "✅ Layer 15 | k_proj | SpSVD successful | shape: torch.Size([4096, 4096])\n",
      "✅ Layer 15 | v_proj | SpSVD successful | shape: torch.Size([4096, 4096])\n",
      "✅ Layer 15 | o_proj | SpSVD successful | shape: torch.Size([4096, 4096])\n",
      "✅ Layer 16 | q_proj | SpSVD successful | shape: torch.Size([4096, 4096])\n",
      "✅ Layer 16 | k_proj | SpSVD successful | shape: torch.Size([4096, 4096])\n",
      "✅ Layer 16 | v_proj | SpSVD successful | shape: torch.Size([4096, 4096])\n",
      "✅ Layer 16 | o_proj | SpSVD successful | shape: torch.Size([4096, 4096])\n",
      "✅ Layer 17 | q_proj | SpSVD successful | shape: torch.Size([4096, 4096])\n",
      "✅ Layer 17 | k_proj | SpSVD successful | shape: torch.Size([4096, 4096])\n",
      "✅ Layer 17 | v_proj | SpSVD successful | shape: torch.Size([4096, 4096])\n",
      "✅ Layer 17 | o_proj | SpSVD successful | shape: torch.Size([4096, 4096])\n",
      "✅ Layer 18 | q_proj | SpSVD successful | shape: torch.Size([4096, 4096])\n",
      "✅ Layer 18 | k_proj | SpSVD successful | shape: torch.Size([4096, 4096])\n",
      "✅ Layer 18 | v_proj | SpSVD successful | shape: torch.Size([4096, 4096])\n",
      "✅ Layer 18 | o_proj | SpSVD successful | shape: torch.Size([4096, 4096])\n",
      "✅ Layer 19 | q_proj | SpSVD successful | shape: torch.Size([4096, 4096])\n",
      "✅ Layer 19 | k_proj | SpSVD successful | shape: torch.Size([4096, 4096])\n",
      "✅ Layer 19 | v_proj | SpSVD successful | shape: torch.Size([4096, 4096])\n",
      "✅ Layer 19 | o_proj | SpSVD successful | shape: torch.Size([4096, 4096])\n",
      "✅ Layer 20 | q_proj | SpSVD successful | shape: torch.Size([4096, 4096])\n",
      "✅ Layer 20 | k_proj | SpSVD successful | shape: torch.Size([4096, 4096])\n",
      "✅ Layer 20 | v_proj | SpSVD successful | shape: torch.Size([4096, 4096])\n",
      "✅ Layer 20 | o_proj | SpSVD successful | shape: torch.Size([4096, 4096])\n",
      "✅ Layer 21 | q_proj | SpSVD successful | shape: torch.Size([4096, 4096])\n",
      "✅ Layer 21 | k_proj | SpSVD successful | shape: torch.Size([4096, 4096])\n",
      "✅ Layer 21 | v_proj | SpSVD successful | shape: torch.Size([4096, 4096])\n",
      "✅ Layer 21 | o_proj | SpSVD successful | shape: torch.Size([4096, 4096])\n",
      "✅ Layer 22 | q_proj | SpSVD successful | shape: torch.Size([4096, 4096])\n",
      "✅ Layer 22 | k_proj | SpSVD successful | shape: torch.Size([4096, 4096])\n",
      "✅ Layer 22 | v_proj | SpSVD successful | shape: torch.Size([4096, 4096])\n",
      "✅ Layer 22 | o_proj | SpSVD successful | shape: torch.Size([4096, 4096])\n",
      "✅ Layer 23 | q_proj | SpSVD successful | shape: torch.Size([4096, 4096])\n",
      "✅ Layer 23 | k_proj | SpSVD successful | shape: torch.Size([4096, 4096])\n",
      "✅ Layer 23 | v_proj | SpSVD successful | shape: torch.Size([4096, 4096])\n",
      "✅ Layer 23 | o_proj | SpSVD successful | shape: torch.Size([4096, 4096])\n",
      "✅ Layer 24 | q_proj | SpSVD successful | shape: torch.Size([4096, 4096])\n",
      "✅ Layer 24 | k_proj | SpSVD successful | shape: torch.Size([4096, 4096])\n",
      "✅ Layer 24 | v_proj | SpSVD successful | shape: torch.Size([4096, 4096])\n",
      "✅ Layer 24 | o_proj | SpSVD successful | shape: torch.Size([4096, 4096])\n",
      "✅ Layer 25 | q_proj | SpSVD successful | shape: torch.Size([4096, 4096])\n",
      "✅ Layer 25 | k_proj | SpSVD successful | shape: torch.Size([4096, 4096])\n",
      "✅ Layer 25 | v_proj | SpSVD successful | shape: torch.Size([4096, 4096])\n",
      "✅ Layer 25 | o_proj | SpSVD successful | shape: torch.Size([4096, 4096])\n",
      "✅ Layer 26 | q_proj | SpSVD successful | shape: torch.Size([4096, 4096])\n",
      "✅ Layer 26 | k_proj | SpSVD successful | shape: torch.Size([4096, 4096])\n",
      "✅ Layer 26 | v_proj | SpSVD successful | shape: torch.Size([4096, 4096])\n",
      "✅ Layer 26 | o_proj | SpSVD successful | shape: torch.Size([4096, 4096])\n",
      "✅ Layer 27 | q_proj | SpSVD successful | shape: torch.Size([4096, 4096])\n",
      "✅ Layer 27 | k_proj | SpSVD successful | shape: torch.Size([4096, 4096])\n",
      "✅ Layer 27 | v_proj | SpSVD successful | shape: torch.Size([4096, 4096])\n",
      "✅ Layer 27 | o_proj | SpSVD successful | shape: torch.Size([4096, 4096])\n",
      "✅ Layer 28 | q_proj | SpSVD successful | shape: torch.Size([4096, 4096])\n",
      "✅ Layer 28 | k_proj | SpSVD successful | shape: torch.Size([4096, 4096])\n",
      "✅ Layer 28 | v_proj | SpSVD successful | shape: torch.Size([4096, 4096])\n",
      "✅ Layer 28 | o_proj | SpSVD successful | shape: torch.Size([4096, 4096])\n",
      "✅ Layer 29 | q_proj | SpSVD successful | shape: torch.Size([4096, 4096])\n",
      "✅ Layer 29 | k_proj | SpSVD successful | shape: torch.Size([4096, 4096])\n",
      "✅ Layer 29 | v_proj | SpSVD successful | shape: torch.Size([4096, 4096])\n",
      "✅ Layer 29 | o_proj | SpSVD successful | shape: torch.Size([4096, 4096])\n",
      "✅ Layer 30 | q_proj | SpSVD successful | shape: torch.Size([4096, 4096])\n",
      "✅ Layer 30 | k_proj | SpSVD successful | shape: torch.Size([4096, 4096])\n",
      "✅ Layer 30 | v_proj | SpSVD successful | shape: torch.Size([4096, 4096])\n",
      "✅ Layer 30 | o_proj | SpSVD successful | shape: torch.Size([4096, 4096])\n",
      "✅ Layer 31 | q_proj | SpSVD successful | shape: torch.Size([4096, 4096])\n",
      "✅ Layer 31 | k_proj | SpSVD successful | shape: torch.Size([4096, 4096])\n",
      "✅ Layer 31 | v_proj | SpSVD successful | shape: torch.Size([4096, 4096])\n",
      "✅ Layer 31 | o_proj | SpSVD successful | shape: torch.Size([4096, 4096])\n"
     ]
    }
   ],
   "source": [
    "pmodel = model_peft\n",
    "proj_modules = ['q_proj', 'k_proj', 'v_proj', 'o_proj']\n",
    "adapter_name = 'default'  # Replace if using another adapter\n",
    "\n",
    "specs, all_singular_values, vh_matrices = [], [], []\n",
    "\n",
    "for layer_idx, layer in enumerate(pmodel.base_model.model.model.layers):\n",
    "    for proj_name in proj_modules:\n",
    "        proj_module = getattr(layer.self_attn, proj_name, None)\n",
    "        if proj_module is not None:\n",
    "            if adapter_name in proj_module.lora_A and adapter_name in proj_module.lora_B:\n",
    "                lora_A = proj_module.lora_A[adapter_name].weight  # [r, in_dim]\n",
    "                lora_B = proj_module.lora_B[adapter_name].weight  # [out_dim, r]\n",
    "                vec = lora_B @ lora_A  # [out_dim, in_dim], e.g., [4096, 4096]\n",
    "\n",
    "                try:\n",
    "                    # Step 1: Row normalization\n",
    "                    row_norms = torch.norm(vec, dim=1, keepdim=True) + 1e-8\n",
    "                    X_row = vec / row_norms  # [out_dim, in_dim]\n",
    "\n",
    "                    # Step 2: Column normalization\n",
    "                    col_norms = torch.norm(vec, dim=0, keepdim=True) + 1e-8\n",
    "                    X_col = vec / col_norms\n",
    "\n",
    "                    # Step 3: SVD on row-normalized matrix (to get Vh)\n",
    "                    _, _, Vh_row = torch.linalg.svd(X_row, full_matrices=False)\n",
    "\n",
    "                    # Step 4: SVD on column-normalized matrix (to get U)\n",
    "                    U_col, _, _ = torch.linalg.svd(X_col, full_matrices=False)\n",
    "\n",
    "                    # Step 5: Search for best (u, v) pair (rank-1 approximation)\n",
    "                    best_score = float('inf')\n",
    "                    best_u, best_v, best_d = None, None, None\n",
    "\n",
    "                    for u in U_col[:5]:  # top-5 vectors only\n",
    "                        for v in Vh_row[:5]:\n",
    "                            d = torch.median((vec @ v) * u)  # robust scalar\n",
    "                            recon = d * torch.ger(u, v)\n",
    "                            score = torch.norm(vec - recon, p=1)\n",
    "                            if score < best_score:\n",
    "                                best_score = score\n",
    "                                best_u, best_v, best_d = u, v, d\n",
    "\n",
    "                    # Step 6: Restore magnitude using row norms \n",
    "                    recon_rank1_normalized = best_d * torch.ger(best_u, best_v)\n",
    "                    recon_rank1_rescaled = row_norms * recon_rank1_normalized  # [out_dim, in_dim]\n",
    "\n",
    "                    # Step 7: Final SVD on rescaled rank-1 matrix\n",
    "                    _, S, Vh_final = torch.linalg.svd(recon_rank1_rescaled, full_matrices=False)\n",
    "\n",
    "                    # Step 8: Store metrics\n",
    "                    specs.append(S[0].item())  # top singular value\n",
    "                    all_singular_values.append(S.cpu().numpy())\n",
    "                    vh_matrices.append(Vh_final)\n",
    "\n",
    "                    print(f\"✅ Layer {layer_idx} | {proj_name} | SpSVD successful | shape: {vec.shape}\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    specs.append(float('nan'))\n",
    "                    print(f\"⚠️ SpSVD failed at layer {layer_idx} {proj_name}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f8b5b99-9fcc-4be8-87b7-2519149adc42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 128, (4096,), 128, torch.Size([4096, 4096]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(specs), len(all_singular_values),all_singular_values[0].shape, len(vh_matrices), vh_matrices[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b867a1f0-97c8-4499-be2b-4508acbed70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Compute SharpIndex for each layer from singular values\n",
    "sharp_indices = []\n",
    "for svals in all_singular_values:\n",
    "    if len(svals) == 0 or np.sum(svals) == 0:\n",
    "        sharp_indices.append(np.nan)\n",
    "    else:\n",
    "        sharp_index = svals[0] / (np.sum(svals) + 1e-6)\n",
    "        sharp_indices.append(sharp_index)\n",
    "\n",
    "sharp_indices = np.array(sharp_indices)\n",
    "\n",
    "# Sort indices of sharpness scores in descending order\n",
    "sorted_indices = np.argsort(-sharp_indices)  # minus sign for descending\n",
    "\n",
    "# Select top-k most \"sharp\" layers\n",
    "k = 5  # or 10, or any value you want\n",
    "outlier_indices = sorted_indices[:k]\n",
    "\n",
    "\n",
    "# Print detected layers\n",
    "print(\"\\n🚨 Detected Outlier Layers by SharpIndex:\")\n",
    "for idx in outlier_indices:\n",
    "    print(f\"⚠️  Layer Index {idx} | SharpIndex = {sharp_indices[idx]:.4f} \")\n",
    "    \n",
    "# Final return or export\n",
    "outlier_indices  # can use in your pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ecb7ac48-6b91-4995-bcfb-b0a5f61cf899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zeroed lora_B in layer 1 | v_proj | global idx 6\n",
      "Zeroed lora_B in layer 19 | q_proj | global idx 76\n",
      "Zeroed lora_B in layer 20 | q_proj | global idx 80\n",
      "Zeroed lora_B in layer 29 | v_proj | global idx 118\n",
      "Zeroed lora_B in layer 31 | v_proj | global idx 126\n"
     ]
    }
   ],
   "source": [
    "outlier_indices = set(outlier_indices)\n",
    "proj_modules = ['q_proj', 'k_proj', 'v_proj', 'o_proj']\n",
    "adapter_name = 'default'\n",
    "\n",
    "global_idx = 0  # to track module position across all layers\n",
    "\n",
    "for layer_idx, layer in enumerate(pmodel.base_model.model.model.layers):\n",
    "    for proj_name in proj_modules:\n",
    "        if global_idx in outlier_indices:\n",
    "            proj_module = getattr(layer.self_attn, proj_name, None)\n",
    "            if proj_module is not None:\n",
    "                if adapter_name in proj_module.lora_B:\n",
    "                    lora_B = proj_module.lora_B[adapter_name].weight\n",
    "                    lora_B.data.zero_()\n",
    "                    print(f\"Zeroed lora_B in layer {layer_idx} | {proj_name} | global idx {global_idx}\")\n",
    "        global_idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a21cb28-432d-4f31-9a48-d6deabd34b20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight, Sparsity: 100.00%\n",
      "Layer: base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight, Sparsity: 100.00%\n",
      "Layer: base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight, Sparsity: 100.00%\n",
      "Layer: base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight, Sparsity: 100.00%\n",
      "Layer: base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight, Sparsity: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# Function to count zero parameters in each layer\n",
    "def count_zero_params(model):\n",
    "    zero_params = {}\n",
    "    for name, param in model.named_parameters():\n",
    "        num_zeros = torch.sum(param == 0).item()\n",
    "        total_params = param.numel()\n",
    "        sparsity = num_zeros / total_params * 100\n",
    "        zero_params[name] = (num_zeros, total_params, sparsity)\n",
    "    return zero_params\n",
    "\n",
    "zero_params_info = count_zero_params(pmodel)\n",
    "\n",
    "# Print layers with high sparsity\n",
    "for layer, (num_zeros, total, sparsity) in zero_params_info.items():\n",
    "    if sparsity > 50:  # Layers with more than 50% zeros\n",
    "        print(f\"Layer: {layer}, Sparsity: {sparsity:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d445de5f-cf36-46da-bbc7-a01d6379f2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.utils.prune as prune\n",
    "\n",
    "# Function to prune layers with high sparsity and return pruned model\n",
    "def prune_zero_params(model, threshold=50):\n",
    "    pruned_model = model  # Work on a copy of the model\n",
    "\n",
    "    for name, module in pruned_model.named_modules():\n",
    "        if hasattr(module, \"weight\") and module.weight is not None:\n",
    "            num_zeros = torch.sum(module.weight == 0).item()\n",
    "            total_params = module.weight.numel()\n",
    "            sparsity = (num_zeros / total_params) * 100\n",
    "\n",
    "            if sparsity > threshold:\n",
    "                # print(f\"Pruning {name} with sparsity {sparsity:.2f}%\")\n",
    "                prune.l1_unstructured(module, name=\"weight\", amount=1.0)  # Fully prune\n",
    "                prune.remove(module, \"weight\")  # Remove redundant params\n",
    "\n",
    "    return pruned_model  # Return the pruned model\n",
    "\n",
    "# Apply pruning and get the new model\n",
    "model_prune = prune_zero_params(pmodel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "69ce6592-f272-4345-8988-77dd3d5fec73",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/sa5u24/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /home/sa5u24/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/sa5u24/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total inference time: 242.32 seconds\n",
      "Average time per sample: 1.2116 seconds\n",
      "ROUGE: {'rouge1': 0.6905914941948734, 'rouge2': 0.5716438218605611, 'rougeL': 0.6082275010307869, 'rougeLsum': 0.6820733395424499}\n",
      "BLEU: {'bleu': 0.5567200191237582, 'precisions': [0.6794777859188703, 0.5906298521568892, 0.521453500307945, 0.45903004232290295], 'brevity_penalty': 1.0, 'length_ratio': 1.0996923527688252, 'translation_length': 15013, 'reference_length': 13652}\n",
      "METEOR: {'meteor': 0.6914506247633411}\n",
      "unc 200\n"
     ]
    }
   ],
   "source": [
    "from evaluate import load\n",
    "import time\n",
    "import torch.nn.functional as F\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "eos_token_id = tokenizer.eos_token_id \n",
    "\n",
    "model_prune = model_prune.eval()\n",
    "\n",
    "def generate_batch(model,dialogues):\n",
    "    \"\"\"\n",
    "    Generate plan using LLaMA2.\n",
    "    \"\"\"\n",
    "\n",
    "    input_texts = [\n",
    "            f\"Follow the instruction: {inst}, input: {inp}, and Output:\"\n",
    "            for inst, inp in zip(dialogues['instruction'], dialogues['input'])\n",
    "        ]\n",
    "    # Tokenize in batch\n",
    "    inputs = tokenizer(input_texts, return_tensors=\"pt\", truncation=True, max_length=512, padding=True).to(model.device)\n",
    "\n",
    "    # Generate summaries\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(**inputs, max_new_tokens=100, pad_token_id=tokenizer.eos_token_id,\n",
    "                                       return_dict_in_generate=True,\n",
    "                                        output_scores=True,\n",
    "                                        output_hidden_states=True,\n",
    "                                        do_sample=True)\n",
    "    \n",
    "    # Trim the generated ids to remove the input ids\n",
    "    trimmed_generated_ids = [\n",
    "        out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids[0])\n",
    "    ]\n",
    "    \n",
    "    scores = [score.cpu() for score in generated_ids[\"scores\"]] \n",
    "    \n",
    "    # Decode and clean up summaries\n",
    "    summaries = tokenizer.batch_decode(generated_ids['sequences'], skip_special_tokens=True)\n",
    "    summaries_final = [s.split(\"Output:\")[-1].strip() for s in summaries]\n",
    "\n",
    "    summaries_trimmed = tokenizer.batch_decode(trimmed_generated_ids, skip_special_tokens=True)\n",
    "    \n",
    "\n",
    "    return summaries_final, summaries_trimmed, trimmed_generated_ids, scores\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "meteor = load(\"meteor\")\n",
    "bleu = load(\"bleu\")\n",
    "rouge = load(\"rouge\")\n",
    "sampled_data = validation_dataset\n",
    "\n",
    "\n",
    "# Process the entire test dataset in batches\n",
    "test_dialogues = sampled_data\n",
    "num_samples = len(test_dialogues)\n",
    "\n",
    "generated_summaries = []\n",
    "samplewise_rouge = []\n",
    "all_scores = []\n",
    "unc_all = []\n",
    "start_time = time.time()\n",
    "for i in range(0, num_samples, BATCH_SIZE):\n",
    "    batch_dialogues = test_dialogues[i : i + BATCH_SIZE]\n",
    "    batch_summaries, batch_summaries_trimmed, trimmed_generated_ids, scores = generate_batch(model_peft,batch_dialogues)\n",
    "    \n",
    "    trimmed_generated_ids_ = torch.stack(trimmed_generated_ids)\n",
    "    # Find positions where EOS token appears (first occurrence per sequence)\n",
    "    eos_positions = (trimmed_generated_ids_ == eos_token_id).int()  # Create a mask\n",
    "    first_eos_indices = torch.argmax(eos_positions, dim=1)  # Find first EOS position in each row\n",
    "    \n",
    "    # Handle cases where EOS is not found (replace 0 with sequence length)\n",
    "    no_eos_mask = (eos_positions.sum(dim=1) == 0)  # Mask sequences without EOS\n",
    "    first_eos_indices[no_eos_mask] = trimmed_generated_ids_.shape[1]  # Assign full length if no EOS found\n",
    "\n",
    "    scores_ = torch.stack(scores).permute(1,0,2)\n",
    "    for i in range(scores_.shape[0]):\n",
    "        valid_scores= scores_[i,:first_eos_indices[i],:]\n",
    "        probs = F.softmax(valid_scores, dim=-1)\n",
    "        entropy = -torch.sum(probs * torch.log(probs + 1e-9), dim=-1) \n",
    "        unc = sum(entropy) / len(entropy) if len(entropy) > 0 else 0.0\n",
    "        unc_all.append(unc)\n",
    "    \n",
    "    # samplewise_rouge.extend(each_rouge)\n",
    "    generated_summaries.extend(batch_summaries_trimmed)\n",
    "    \n",
    "end_time = time.time()\n",
    "\n",
    "# Compute Inference Time\n",
    "total_time = end_time - start_time\n",
    "avg_time_per_sample = total_time / num_samples\n",
    "\n",
    "print(f\"Total inference time: {total_time:.2f} seconds\")\n",
    "print(f\"Average time per sample: {avg_time_per_sample:.4f} seconds\")\n",
    "\n",
    "# Load the metrics\n",
    "\n",
    "test_ans = sampled_data[\"output\"]\n",
    "all_pred = generated_summaries\n",
    "all_ans = test_ans\n",
    "\n",
    "rouge_results = rouge.compute(predictions=all_pred, references=all_ans)\n",
    "each_rouge = rouge.compute(predictions=all_pred, references=all_ans, use_aggregator=False)\n",
    "\n",
    "bleu_result = bleu.compute(predictions=all_pred, references=all_ans)\n",
    "meteor_result = meteor.compute(predictions=all_pred, references=all_ans)\n",
    "\n",
    "print(\"ROUGE:\", rouge_results)\n",
    "print(\"BLEU:\", bleu_result)\n",
    "print(\"METEOR:\", meteor_result)\n",
    "print(\"unc\", len(unc_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fa2050c2-894b-40bc-bf0c-71e49a6326f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.89"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# proxy accuracy\n",
    "# each_rouge = rouge.compute(predictions=all_pred, references=all_ans, use_aggregator=False)\n",
    "all_acc = []\n",
    "for i in range(len(each_rouge['rouge1'])):\n",
    "    if each_rouge['rouge1'][i]>0.5:\n",
    "        acc = 1\n",
    "    else:\n",
    "        acc = 0\n",
    "    all_acc.append(acc)\n",
    "\n",
    "overall_acc = sum(all_acc)/len(all_acc)\n",
    "overall_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "487a7581-a262-4da1-9db1-f7844f7ce97a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5919305413687437, 0.9054302668971186)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "unc_all_ = unc_all[::-1]\n",
    "auroc = roc_auc_score(all_acc, unc_all_)\n",
    "auarc = average_precision_score(all_acc, unc_all_)\n",
    "auroc,auarc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a58922-e8b8-4d5e-8c42-cc1e8b9c9448",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61222ce-3b6e-45ad-9df9-d3fae0d6e9c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
